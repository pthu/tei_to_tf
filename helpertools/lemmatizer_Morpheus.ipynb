{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================#\n",
    "# Author:      Ernst Boogert                       #\n",
    "# Institution: Protestant Theological University   #\n",
    "# Date:        August 27, 2018                     #\n",
    "# Version:     0.0.2                               #\n",
    "#==================================================#\n",
    "\n",
    "#==============Documentation============================#\n",
    "# It should be noted what this lemmatizer does...       \n",
    "# It takes the MorpheusUnicode data as starting point.  \n",
    "# Next it builds a dictionary with all concrete words   \n",
    "# in this database and gives all possible lemmata as     \n",
    "# values in a set.\n",
    "# Next, the lemmatizer lemmatizes tokens according to\n",
    "# this dictionary, which means that not the best lemma\n",
    "# in context is given, but a set with all possibilities\n",
    "# according to the MorpheusUnicode database.\n",
    "# This lemmatizer is best used in tasks like textcomparison.\n",
    "#==============End Documentation========================#\n",
    "\n",
    "from lxml import etree\n",
    "import unicodedata as ud\n",
    "import pickle\n",
    "import zipfile\n",
    "\n",
    "#%run normalization_accents.ipynb\n",
    "%run tokenizer.ipynb\n",
    "\n",
    "lemmatizer_dictionary = {}\n",
    "\n",
    "def buildLemmatizerDictionary(): \n",
    "    # Unzip datafile\n",
    "    with zipfile.ZipFile(\"data/MorpheusUnicode.xml.zip\", 'r') as lemmatizer_zipfile:\n",
    "        lemmatizer_zipfile.extractall(\"data\")\n",
    "        utf8_parser = etree.XMLParser(encoding='utf-8') # Make sure that the XML is parsed in utf-8 encoding\n",
    "        tree = etree.parse(\"data/MorpheusUnicode.xml\", parser=utf8_parser)\n",
    "    \n",
    "    # Process loop to find all the word-lemma units, extract the text and store it in a dictionary\n",
    "    # while avoiding any duplicate key and value data\n",
    "        for unit in tree.iter('t'):\n",
    "            form1 = strip_accents(unit.findtext('f'))\n",
    "            lemma = strip_accents(unit.findtext('l'))\n",
    "            #word = ud.normalize(\"NFD\", unit.findtext('f'))\n",
    "            #lemma = ud.normalize(\"NFD\", unit.findtext('l'))\n",
    "            \n",
    "#             if lemma not in lemmatizer_dictionary:\n",
    "#                 lemmatizer_dictionary.update({lemma:{word}})\n",
    "#             else:\n",
    "#                 lemmatizer_dictionary[lemma].add(word)               # Makes set values of lemmata\n",
    "            if word not in lemmatizer_dictionary:                  # Word = key, lemma = value\n",
    "                lemmatizer_dictionary.update({word:{lemma}})\n",
    "            else:\n",
    "                lemmatizer_dictionary[word].add(lemma)               # Makes set values of lemmata\n",
    "        return lemmatizer_dictionary\n",
    "    \n",
    "\n",
    "def buildDictionaryPickle(data):\n",
    "    pickle.dump(data, open(\"data/lemmatizer_dict.pickle\", \"wb\"))\n",
    "    \n",
    "# # Execute functions\n",
    "#buildLemmatizerDictionary()\n",
    "#buildDictionaryPickle(lemmatizer_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('λεγω',),\n",
       " ('αυτεω', 'εαυτου', 'αυτος'),\n",
       " ('τιω', 'ετι', 'τις', 'τιο'),\n",
       " ('ζητεω',),\n",
       " ('οιις', 'ο', 'ε', 'ιημι', 'οιος', 'ος', 'οις'),\n",
       " ('δει', 'δεω', 'δε'),\n",
       " ('ειπον',),\n",
       " ('αυτεω', 'αυτου', 'εαυτου', 'αυτος'),\n",
       " 'ραββει',\n",
       " ('ος', 'ο'),\n",
       " ('λεγω',),\n",
       " 'μεθερμηνευομενον',\n",
       " ('διδασκαλος',),\n",
       " ('που',),\n",
       " ('μενω',)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import unicodedata as ud\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "#token_list = (\"κεκρότηται\", \"κρηπὶς\", \"ἀληθείας\", \"ὦ\", \"παῖδες\", \"ὑμεῖς\", \"ἡμῖν\", \"αὐτοῖς\", \"ἁγίου\", \"νεὼ\", \"μεγάλου\", \"θεοῦ\", \"θεμέλιος\", \"γνώσεως\", \"ἀρραγής\", \"προτροπὴ\")\n",
    "greek_token_list = ('λέγει',\n",
    " 'αὐτοῖς',\n",
    " 'Τί',\n",
    " 'ζητεῖτε',\n",
    " 'οἱ',\n",
    " 'δὲ',\n",
    " 'εἶπαν',\n",
    " 'αὐτῷ',\n",
    " 'Ῥαββεί',\n",
    " 'ὃ',\n",
    " 'λέγεται',\n",
    " 'μεθερμηνευόμενον',\n",
    " 'Διδάσκαλε',\n",
    " 'ποῦ',\n",
    " 'μένεις')\n",
    "\n",
    "def lemmatizer_dict(lemma_dict, token_list):\n",
    "    lemmatized_dict = {}\n",
    "    for token in token_list:\n",
    "        #token = ud.normalize(\"NFC\", token)\n",
    "        token_stripped = strip_accents(token)\n",
    "        if token_stripped in lemma_dict:\n",
    "            lemmatized_dict.update({token:lemma_dict.get(token_stripped)})\n",
    "        else:\n",
    "            lemmatized_dict.update({token:{token_stripped}})\n",
    "    return lemmatized_dict\n",
    "\n",
    "def lemmatizer(lemma_dict, token_list):\n",
    "    lemmatized_list = []\n",
    "    for token in token_list:\n",
    "        #token = ud.normalize(\"NFC\", token)\n",
    "        token_stripped = strip_accents(token)\n",
    "        if token_stripped in lemma_dict:\n",
    "            lemmatized_list.append(tuple(lemma_dict.get(token_stripped)))\n",
    "        else:\n",
    "            lemmatized_list.append(token_stripped)\n",
    "    return lemmatized_list\n",
    "\n",
    "#     for word, lemmata in lemmatized_dict.items(): # Check whether a lemma exists in the value sets\n",
    "#         if 'ιημι' in lemmata:\n",
    "#             print(word)\n",
    "#         else:\n",
    "#             pass\n",
    "#    lemmatized_dict.keys() # returns a list with all dictionary keys!\n",
    "#    lemmatized_dict.values() # returns a list with all the value sets!\n",
    "    \n",
    "\n",
    "lemmatizer( pickle.load( open (\"data/lemmatizer_dict.pickle\", \"rb\") ), greek_token_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
