{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lemmatizer of Greek text\n",
    "\n",
    "This file provides a function (createLemmatizer) that creates (pickles) a dictionary that contains Greek wordforms as its keys and its possible lemmata as values (set). It also provides a function (lemmatize) that takes two arguments: a wordstring and the just created dictionary of lemmata. It returns a string with the possible lemmata in comma-separated format.\n",
    "\n",
    "If you like to use these functions, be aware to load the lemmatizer (=lemma dictionary) only once...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from unicodedata import normalize, category\n",
    "from os import path\n",
    "from pprint import pprint\n",
    "import xml.etree.ElementTree as etree\n",
    "from tf.fabric import Timestamp\n",
    "import nbimporter\n",
    "\n",
    "udnorm = 'NFD'\n",
    "\n",
    "REPO = '~/github/pthu/tei_to_tf/'\n",
    "SRC_DIR = path.expanduser(f'{REPO}helpertools/data')\n",
    "SOURCE1 = SRC_DIR + '/forms-normalised-20180208_001.txt'\n",
    "SOURCE2 = SRC_DIR + '/MorpheusUnicode.xml'\n",
    "\n",
    "letter = {'L'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_FORMS = {\n",
    "    'θεὸς': 'θεός',\n",
    "    'ἰδοὺ': 'ἰδοὺ',\n",
    "    'ἐμὲ': 'ἐμός',\n",
    "    'τάυτην': 'οὗτος',\n",
    "    'εὐλογημένος': 'εὐλογέω',\n",
    "    'δαβίδ': 'δαβίδ',\n",
    "    'ὡσαννά': 'ὡσαννά',\n",
    "    'ἐγὼ': 'ἐγώ',\n",
    "    'μωσέως': 'μωσέως',\n",
    "    'ἱερουσαλήμ': 'ἱερουσαλήμ',\n",
    "    'ἔρχεταί': 'ἔρχομαι',\n",
    "    'ἡσαΐου': 'ἡσαΐου',\n",
    "    'ἰσαὰκ': 'ἰσαὰκ'\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(word):\n",
    "    return ''.join(c for c in normalize(udnorm, word.lower())\n",
    "                   if category(c)[0] in letter)\n",
    "\n",
    "def createLemmatizer(sourcepath1, sourcepath2):\n",
    "    lemma_dict = {}\n",
    "    with open(sourcepath1) as source1:\n",
    "        for line in source1:\n",
    "            form, alternative, morphology, lemma = line.strip().split(\"\\t\")\n",
    "            form1 = normalize(udnorm, form.lower())\n",
    "            form2 = strip_accents(form1)\n",
    "            alternative1 = normalize(udnorm, alternative.lower())\n",
    "            alternative2 = strip_accents(alternative1)\n",
    "            lemma = normalize(udnorm, lemma.lower())\n",
    "            if form1 in lemma_dict:\n",
    "                lemma_dict[form1].add(lemma)\n",
    "            else:\n",
    "                lemma_dict[form1] = {lemma}\n",
    "            if form2 in lemma_dict:\n",
    "                lemma_dict[form2].add(lemma)\n",
    "            else:\n",
    "                lemma_dict[form2] = {lemma}\n",
    "                \n",
    "            if alternative1 in lemma_dict:\n",
    "                lemma_dict[alternative1].add(lemma)\n",
    "            else:\n",
    "                lemma_dict[alternative1] = {lemma}\n",
    "            if alternative2 in lemma_dict:\n",
    "                lemma_dict[alternative2].add(lemma)\n",
    "            else:\n",
    "                lemma_dict[alternative2] = {lemma}\n",
    "                \n",
    "    with open(sourcepath2) as source2:\n",
    "        tree = etree.parse(source2)\n",
    "        for elem in tree.iter('t'):\n",
    "            form1a = normalize(udnorm, elem.findtext('f').lower())\n",
    "            form2a = normalize(udnorm, elem.findtext('b').lower())\n",
    "            form1b = strip_accents(elem.findtext('f').lower())\n",
    "            form2b = strip_accents(elem.findtext('b').lower())\n",
    "            lemma = normalize(udnorm, elem.findtext('l').lower())\n",
    "            if form1a in lemma_dict:\n",
    "                lemma_dict[form1a].add(lemma)\n",
    "            else:\n",
    "                lemma_dict[form1a] = {lemma}\n",
    "            if form2a in lemma_dict:\n",
    "                lemma_dict[form2a].add(lemma)\n",
    "            else:\n",
    "                lemma_dict[form2a] = {lemma}\n",
    "                \n",
    "            if form1b in lemma_dict:\n",
    "                lemma_dict[form1b].add(lemma)\n",
    "            else:\n",
    "                lemma_dict[form1b] = {lemma}\n",
    "            if form2b in lemma_dict:\n",
    "                lemma_dict[form2b].add(lemma)\n",
    "            else:\n",
    "                lemma_dict[form2b] = {lemma}\n",
    "        tree = None\n",
    "        \n",
    "    from data.greek_lemmata_cltk import LEMMATA\n",
    "        \n",
    "    for key, value in LEMMATA.items():\n",
    "        formA = normalize(udnorm, key.lower())\n",
    "        formB = strip_accents(key.lower())\n",
    "        lemma = normalize(udnorm, value.lower())\n",
    "        if formA in lemma_dict:\n",
    "            lemma_dict[formA].add(lemma)\n",
    "        else:\n",
    "            lemma_dict[formA] = {lemma}\n",
    "        if formB in lemma_dict:\n",
    "            lemma_dict[formB].add(lemma)\n",
    "        else:\n",
    "            lemma_dict[formB] = {lemma}\n",
    "            \n",
    "    for key, value in MANUAL_FORMS.items():\n",
    "        formA = normalize(udnorm, key.lower())\n",
    "        formB = strip_accents(key.lower())\n",
    "        lemma = normalize(udnorm, value.lower())\n",
    "        if formA in lemma_dict:\n",
    "            lemma_dict[formA].add(lemma)\n",
    "        else:\n",
    "            lemma_dict[formA] = {lemma}\n",
    "        if formB in lemma_dict:\n",
    "            lemma_dict[formB].add(lemma)\n",
    "        else:\n",
    "            lemma_dict[formB] = {lemma}\n",
    "    \n",
    "    # Handle movable-nu and final-sigma\n",
    "    lemma_dict_add = {}\n",
    "    for wordform in lemma_dict:\n",
    "        if strip_accents(wordform).endswith(('εν', 'σιν', 'στιν')) and \\\n",
    "                                        len(strip_accents(wordform)) > 2:\n",
    "            if wordform[:-1] in lemma_dict:\n",
    "                pass\n",
    "            else:\n",
    "                lemma_dict_add[wordform[:-1]] = lemma_dict[wordform]\n",
    "        elif wordform.endswith('σ'):\n",
    "            if wordform[:-1] + 'ς' in lemma_dict:\n",
    "                pass\n",
    "            else:\n",
    "                lemma_dict_add[wordform[:-1] + 'ς'] = lemma_dict[wordform]\n",
    "    lemma_dict.update(lemma_dict_add)\n",
    "    \n",
    "    with open(SRC_DIR + '/lemmatizer.pickle', 'wb') as lemmatizer:\n",
    "        pickle.dump(lemma_dict, lemmatizer, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Run the creation process    \n",
    "createLemmatizer(SOURCE1, SOURCE2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize(word, lemmatizer):\n",
    "    word = normalize('NFD', word.lower())\n",
    "    if word in lemmatizer:\n",
    "        word = ','.join(lemmatizer[word])\n",
    "    else:\n",
    "        word = f'*{word}'\n",
    "    return word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small test setup\n",
    "\n",
    "# lemmatizer_open = open(SRC_DIR + '/lemmatizer.pickle', 'rb')\n",
    "# lemmatizer = pickle.load(lemmatizer_open)\n",
    "\n",
    "# selection = {k: v for k, v in sorted(lemmatizer.items())[:100]}\n",
    "# pprint(selection)\n",
    "# pprint(f'The total number of available wordforms = {len(lemmatizer)}')\n",
    "\n",
    "# lemmatize('ἐΠράχθη', lemmatizer)\n",
    "# lemmatizer_open.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
